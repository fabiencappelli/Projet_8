{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","mount_file_id":"1id2SLSUjyP3NMFd7Cx1XsDYQXl-Sj45e","authorship_tag":"ABX9TyNXyBSc1+SGfNUbrOBfndqq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports et variables"],"metadata":{"id":"f3mPeaGxctiJ"}},{"cell_type":"code","source":["!pip install segmentation-models dagshub mlflow"],"metadata":{"id":"mP8MOPYUe6QT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import time\n","from google.colab import userdata\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import albumentations as A\n","import shutil\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","import dagshub\n","import mlflow\n","import pickle\n","\n","# Set the SM_FRAMEWORK environment variable before importing segmentation_models\n","os.environ['SM_FRAMEWORK'] = 'tf.keras'\n","import segmentation_models as sm\n","\n","# Récupère automatiquement le secret\n","dagshub_token = userdata.get('DAGSHUB_TOKEN')\n","\n","# Initialisation Dagshub\n","dagshub.auth.add_app_token(dagshub_token)\n","\n","# Connecter MLflow à Dagshub\n","dagshub.init(repo_owner='fabiencappelli', repo_name='Projet_08', mlflow=True)\n","\n","from matplotlib import rcParams\n","import matplotlib.font_manager as fm\n","import matplotlib.ticker as mticker\n","font_path = os.path.expanduser(\"/content/drive/MyDrive/Colab Notebooks/fonts/Exo2-VariableFont_wght.ttf\")\n","fm.fontManager.addfont(font_path)\n","\n","# Définir la police globale avec le nom de la police\n","rcParams[\"font.family\"] = \"Exo 2\"\n","# deux couleurs pertinentes pour aller avec la présentation\n","bleuclair = (0.15, 0.55, 0.82)\n","couleur_complementaire = (1 - bleuclair[0], 1 - bleuclair[1], 1 - bleuclair[2])\n","bleufonce = \"#073642\"\n","\n","from sklearn.model_selection import train_test_split\n","import glob\n"],"metadata":{"id":"KlnFbaFWcwlq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imgPrezPath = '/content/drive/MyDrive/Colab Notebooks/Projet 8/img'"],"metadata":{"id":"RCVgkdr9N9c-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","for gpu in gpus:\n","  tf.config.experimental.set_memory_growth(gpu, True)"],"metadata":{"id":"SvGnDUB-V7cw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Je vais charger mes fichiers sur le drive du Colab plutôt que sur le Google Drive monté sur l'environnement Colab : gain de temps testé à x2"],"metadata":{"id":"SWRZ3z7T86hB"}},{"cell_type":"code","source":["masksSourceFolder = '/content/drive/MyDrive/Colab Notebooks/Projet 8/P8_Cityscapes_gtFine_trainvaltest/gtFine8'\n","imagesSourceFolder = '/content/drive/MyDrive/Colab Notebooks/Projet 8/P8_Cityscapes_leftImg8bit_trainvaltest/leftImg8bit'"],"metadata":{"id":"Pp0W-Yl1c70U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masksFolder = '/content/images'\n","imagesFolder = '/content/masks'"],"metadata":{"id":"H1fhCJxcQ-3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sources = [masksSourceFolder, imagesSourceFolder]\n","destinations = [masksFolder, imagesFolder]"],"metadata":{"id":"zh9Dsx46SMki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splits = ['train', 'val']"],"metadata":{"id":"1ks5SCS1Rbdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for split in splits:\n","  for source, destination in zip(sources, destinations):\n","    src_dir = os.path.join(source, split)\n","    dst_dir = os.path.join(destination, split)\n","    os.makedirs(destination, exist_ok=True)\n","    os.makedirs(dst_dir, exist_ok=True)\n","    for fichier in os.listdir(src_dir):\n","      if fichier.endswith(\"labelIds.png\") or fichier.endswith(\"leftImg8bit.png\"):\n","        shutil.copyfile(os.path.join(src_dir, fichier), os.path.join(dst_dir, fichier))"],"metadata":{"id":"yHGwzgAaRjhI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_dir = os.path.join(imagesFolder, 'train')\n","y_train_dir = os.path.join(masksFolder, 'train')\n","\n","X_val_dir = os.path.join(imagesFolder, 'val')\n","y_val_dir = os.path.join(masksFolder, 'val')\n","\n","train_imgs = sorted(glob.glob(os.path.join(X_train_dir, \"*_leftImg8bit.png\")))\n","train_masks = sorted(glob.glob(os.path.join(y_train_dir, \"*_gtFine_labelIds.png\")))\n","\n","assert len(train_imgs) == len(train_masks), \"Mismatch images/masks\"\n","\n","# Split 90% train / 10% test (val déjà à part)\n","train_imgs_new, test_imgs_new, train_masks_new, test_masks_new = train_test_split(\n","    train_imgs, train_masks, test_size=0.1, random_state=34\n",")\n","\n","os.makedirs(os.path.join(imagesFolder, 'test'), exist_ok=True)\n","os.makedirs(os.path.join(masksFolder, 'test'), exist_ok=True)\n","\n","X_test_dir = os.path.join(imagesFolder, 'test')\n","y_test_dir = os.path.join(masksFolder, 'test')\n","\n","for img, mask in zip(test_imgs_new, test_masks_new):\n","    shutil.move(img, X_test_dir)\n","    shutil.move(mask, y_test_dir)"],"metadata":{"id":"R9GdqE5pdgFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dicoclasses = {0:'void',\n","               1:'flat',\n","               2:'construction',\n","               3:'object',\n","               4:'nature',\n","               5:'sky',\n","               6:'human',\n","               7:'vehicle',\n","              }"],"metadata":{"id":"yJwkact8INyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Loader"],"metadata":{"id":"8kv8GEDVb8Qy"}},{"cell_type":"markdown","source":["https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n","\n","https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb"],"metadata":{"id":"kDPoxq2Sb5oh"}},{"cell_type":"code","source":["# je dois faire du one hot encoding sur le mask pour pouvoir utiliser les métriques dans la compilation du modèle\n","def to_onehot(mask, num_classes=8):\n","    return tf.one_hot(mask, num_classes).numpy().astype(np.float32)"],"metadata":{"id":"iuOD-II0MJmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adapted from https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb\n","class Dataloader(tf.keras.utils.Sequence):\n","    \"\"\"Load data from dataset and form batches\n","\n","    Args:\n","        data_folder: folder where is data.\n","        batch_size: Integet number of images in batch.\n","        transform: albumentations.Compose.\n","        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n","    \"\"\"\n","\n","    def __init__(self, data_folder, batch_size=1, transform=None, shuffle=False):\n","        self.data_folder = data_folder\n","        if data_folder == X_train_dir:\n","            self.mask_folder = y_train_dir\n","        elif data_folder == X_val_dir:\n","            self.mask_folder = y_val_dir\n","        elif data_folder == X_test_dir:\n","            self.mask_folder = y_test_dir\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.transform = transform\n","        self.indexes = list(set([path[:path.rfind('_')] for path in os.listdir(data_folder) if path.endswith(\".png\") ]))\n","        self.mask_indexes = [os.path.join(self.mask_folder, path) for path in self.indexes]\n","        self.indexes = [os.path.join(data_folder, path) for path in self.indexes]\n","\n","        self.on_epoch_end()\n","\n","    def __getitem__(self, i):\n","\n","        # collect batch data\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        images = []\n","        masks = []\n","        for j in range(start, stop):\n","            root_file = self.indexes[j]\n","            mask_file = self.mask_indexes[j]\n","            image_file = root_file + \"_leftImg8bit.png\"\n","            mask_file = mask_file + \"_gtFine_labelIds.png\"\n","            # on convertit les images en array numpy\n","            image = np.array(Image.open(image_file))\n","            mask = np.array(Image.open(mask_file))\n","            # Appliquer la transformation si elle est demandée\n","            if self.transform is not None:\n","                transformed = self.transform(image=image, mask=mask)\n","                image = transformed[\"image\"]\n","                mask = transformed[\"mask\"]\n","            # c'est à ce moment qu'on encode en one-hot\n","            mask = to_onehot(mask)\n","            # on les ajoute à leurs listes respectives\n","            images.append(image)\n","            masks.append(mask)\n","        # transpose list of lists\n","        # batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n","        image_batch = np.stack(images, axis=0)\n","        mask_batch = np.stack(masks, axis=0)\n","        # print(\"DEBUG | images\", image_batch.shape, image_batch.dtype)\n","        # print(\"DEBUG | masks \", mask_batch.shape, mask_batch.dtype)\n","        # print(\"DEBUG | unique mask values\", np.unique(mask_batch))\n","        return image_batch, mask_batch\n","\n","    def __len__(self):\n","        \"\"\"Denotes the number of batches per epoch\"\"\"\n","        return len(self.indexes) // self.batch_size\n","        # return int(np.ceil(len(self.indexes) / self.batch_size))\n","\n","\n","    def on_epoch_end(self):\n","        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n","        if self.shuffle:\n","          # On mélange les paires image/mask ENSEMBLE\n","          combined = list(zip(self.indexes, self.mask_indexes))\n","          np.random.shuffle(combined)\n","          self.indexes, self.mask_indexes = zip(*combined)\n","          # zip(*combined) retourne des tuples, donc si tu veux des listes :\n","          self.indexes = list(self.indexes)\n","          self.mask_indexes = list(self.mask_indexes)"],"metadata":{"id":"yPaiatyLec_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Il va falloir ajouter la data augmentation et le preprocessing"],"metadata":{"id":"z90SXENIsusp"}},{"cell_type":"markdown","source":["https://albumentations.ai/docs/3-basic-usage/semantic-segmentation/"],"metadata":{"id":"o4WF1bBuL279"}},{"cell_type":"markdown","source":["https://albumentations.ai/docs/3-basic-usage/choosing-augmentations/"],"metadata":{"id":"UkZkBnty3269"}},{"cell_type":"markdown","source":["https://albumentations.ai/docs/api-reference/albumentations/augmentations/geometric/transforms/#ShiftScaleRotate"],"metadata":{"id":"ak0w8Thl8mVa"}},{"cell_type":"code","source":["train_transform = A.Compose([\n","    # 1. Cropping / Resize\n","    A.Resize(256, 512),\n","\n","    # 2. Basic Geometric (invariances basiques)\n","    A.HorizontalFlip(p=0.5),\n","    # Pas de flip vertical, pas de symétrie carrée (sauf imagerie satellite)\n","\n","    # 3. Dropout/Occlusion (pour la robustesse aux obstacles)\n","    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.2),\n","\n","    # 4. Color/Channel dropout (si tu veux vraiment rendre le modèle insensible à la couleur)\n","    A.ToGray(p=0.1),\n","    A.ChannelDropout(p=0.1),\n","\n","    # 5. Affine transformations\n","    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=5, p=0.25),\n","\n","    # 6. Domain-Specific (effets météo, soleil, etc.)\n","    # A.RandomSunFlare(p=0.1),\n","    # A.RandomShadow(p=0.1),\n","    # A.RandomFog(p=0.05),\n","    # A.RandomRain(p=0.05),\n","    # A.RandomSnow(p=0.05),\n","    # Autres effets spécifiques :\n","    # A.RandomBrightnessContrast(p=0.2),\n","    A.GaussNoise(p=0.1),\n","\n","    # 7. Normalization (toujours à la fin)\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n","])\n","\n","\n","val_transform = A.Compose([\n","    A.Resize(256, 512),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n","])"],"metadata":{"id":"-KM8STIBsuEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = Dataloader(\n","    data_folder=X_train_dir,\n","    batch_size=8,\n","    transform=train_transform,\n","    shuffle=True\n",")\n","\n","val_loader = Dataloader(\n","    data_folder=X_val_dir,\n","    batch_size=8,\n","    transform=val_transform,\n","    shuffle=False\n",")"],"metadata":{"id":"ytBW4TlNQPgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fonctions de visualisation pour vérifier le comportement de l'augmentation\n","# Simple function to overlay mask on image for visualization\n","def overlay_mask(image, mask, alpha=0.5, color=(0, 1, 0)): # Green overlay\n","    # Convert mask to 3 channels if needed, ensure boolean type\n","    mask_overlay = np.zeros_like(image, dtype=np.uint8)\n","    # Create a color overlay where mask is > 0\n","    mask_overlay[mask > 0] = (np.array(color) * 255).astype(np.uint8)\n","\n","    # Blend image and overlay\n","    overlayed_image = cv2.addWeighted(image, 1, mask_overlay, alpha, 0)\n","    return overlayed_image\n","\n","def visualize_segmentation(dataset, idx=0, samples=3):\n","    import matplotlib.pyplot as plt\n","\n","    if isinstance(dataset.transform, A.Compose):\n","        vis_transform_list = [\n","            t for t in dataset.transform\n","            if not isinstance(t, (A.Normalize, A.ToTensorV2))\n","        ]\n","        vis_transform = A.Compose(vis_transform_list)\n","    else:\n","        print(\"Warning: Could not automatically strip Normalize/ToTensor for visualization.\")\n","        vis_transform = dataset.transform\n","\n","    figure, ax = plt.subplots(samples + 1, 2, figsize=(8, 4 * (samples + 1)))\n","\n","    # --- Get the original image and mask --- #\n","    original_transform = dataset.transform\n","    dataset.transform = None # Temporarily disable for raw data access\n","    image_batch, mask_batch = dataset[idx]\n","    image = image_batch[0]   # Prends la première image du batch\n","    mask = mask_batch[0]     # Prends le premier masque du batch\n","    dataset.transform = original_transform # Restore\n","\n","    # --- Patch : assure l'image est (H,W,3) uint8 --- #\n","    if image.ndim == 2: # grayscale\n","        image = np.stack([image]*3, axis=-1)\n","    if image.ndim == 3 and image.shape[2] == 1:\n","        image = np.repeat(image, 3, axis=2)\n","    if image.dtype != np.uint8:\n","        image = image.astype(np.uint8)\n","    # --- Patch masque (H,W) --- #\n","    if mask.ndim == 3 and mask.shape[-1] > 1:\n","        mask_disp = np.argmax(mask, axis=-1)\n","    else:\n","        mask_disp = mask\n","    ax[0, 0].imshow(image)\n","    ax[0, 0].set_title(\"Original Image\")\n","    ax[0, 0].axis(\"off\")\n","    ax[0, 1].imshow(mask_disp, cmap='tab20')  # 'tab20' pour mieux distinguer les classes\n","    ax[0, 1].set_title(\"Original Mask\")\n","    ax[0, 1].axis(\"off\")\n","\n","    # --- Apply and display augmented versions --- #\n","    for i in range(samples):\n","        # Applique la transformation de visu\n","        if vis_transform:\n","            augmented = vis_transform(image=image, mask=mask)\n","            aug_image = augmented['image']\n","            aug_mask = augmented['mask']\n","        else:\n","            aug_image, aug_mask = image, mask\n","\n","        # PATCH : force format image/mask\n","        if aug_image.ndim == 2:\n","            aug_image = np.stack([aug_image]*3, axis=-1)\n","        if aug_image.ndim == 3 and aug_image.shape[2] == 1:\n","            aug_image = np.repeat(aug_image, 3, axis=2)\n","        if aug_image.dtype != np.uint8:\n","            aug_image = aug_image.astype(np.uint8)\n","        if aug_mask.ndim == 3:\n","            aug_mask = aug_mask.squeeze()\n","        if aug_mask.ndim == 3 and aug_mask.shape[-1] > 1:\n","            aug_mask = np.argmax(aug_mask, axis=-1)\n","        else:\n","            aug_mask = aug_mask\n","\n","\n","        ax[i + 1, 0].imshow(aug_image)\n","        ax[i + 1, 0].set_title(f\"Augmented Image {i+1}\")\n","        ax[i + 1, 0].axis(\"off\")\n","\n","        ax[i + 1, 1].imshow(aug_mask, cmap='tab20')\n","        ax[i + 1, 1].set_title(f\"Augmented Mask {i+1}\")\n","        ax[i + 1, 1].axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(imgPrezPath, \"augmentation_visualizer.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)\n","\n"],"metadata":{"id":"kDYVerD-QcHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_segmentation(train_loader, samples=3)"],"metadata":{"id":"qsJm57WMQj4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_batch(dataset, idx=0, n=5, class_labels=None):\n","    \"\"\"\n","    Visualise n paires image/masque d’un batch donné du DataLoader.\n","    \"\"\"\n","    # On récupère un batch d'index idx (un lot d'images et de masques)\n","    images, masks = dataset[idx]\n","    # images.shape = (batch_size, H, W, C)\n","    # on extrait donc la batch size ainsi\n","    batch_size = images.shape[0]\n","    # on fait attention, si le batch a moins d'élements que de paires demandées, on recalibre le nombre de paires demandées\n","    n = min(n, batch_size)\n","\n","    # on va faire les subplots pour toutes les paires images et masks\n","\n","    fig, axs = plt.subplots(n, 2, figsize=(8, 3 * n))\n","    if n == 1: axs = [axs]  # Pour le cas batch de 1\n","\n","    for i in range(n):\n","        # colonne de gauche : image\n","        plt.subplot(n, 2, 2 * i + 1)\n","        img = images[i]\n","        # Si l’image est grayscale (juste 2D, shape (H, W)), on la duplique sur 3 canaux pour obtenir (H, W, 3)\n","        if img.ndim == 2:\n","            img = np.stack([img]*3, axis=-1)\n","        # Si l’image est au format (H, W, 1) (toujours grayscale mais avec une dimension \"canal\" explicite), on la répète sur 3 canaux pour obtenir (H, W, 3)\n","        if img.ndim == 3 and img.shape[2] == 1:\n","            img = np.repeat(img, 3, axis=2)\n","        if img.dtype != np.uint8:\n","            img = img.astype(np.uint8)\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","        plt.title(f\"Image {i}\")\n","        # colonne de droite : mask\n","        plt.subplot(n, 2, 2 * i + 2)\n","        mask = masks[i]\n","        if mask.ndim == 3 and mask.shape[-1] > 1:\n","            mask_disp = np.argmax(mask, axis=-1)\n","        else:\n","            mask_disp = mask\n","        im = axs[i][1].imshow(mask_disp, cmap='tab20', vmin=0, vmax=len(class_labels)-1 if class_labels else None)\n","        axs[i][1].axis(\"off\")\n","        axs[i][1].set_title(f\"Mask {i}\")\n","    plt.tight_layout()\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(imgPrezPath, \"batch_shower.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)\n"],"metadata":{"id":"LdvCvCwpUnMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_batch(train_loader, idx=0, n=5, class_labels=dicoclasses)"],"metadata":{"id":"Y9RLxxdOUpw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_batch(train_loader, idx=4, n=7, class_labels=dicoclasses)"],"metadata":{"id":"j3uFJfW9eQS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline\n","\n","https://github.com/zhixuhao/unet/blob/master/model.py\n","\n","Je vais devoir modifier à la fois l'input et la sortie\n","\n","https://keras.io/api/metrics/segmentation_metrics/\n","\n"],"metadata":{"id":"oEWFrNRy_kbm"}},{"cell_type":"code","source":["import skimage.io as io\n","import skimage.transform as trans\n","\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler"],"metadata":{"id":"rbv0HrG7AxPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def unet(pretrained_weights = None,input_size = (256,512,3)):\n","    inputs = Input(input_size)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","    drop4 = Dropout(0.5)(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","    drop5 = Dropout(0.5)(conv5)\n","\n","    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","    merge6 = concatenate([drop4,up6], axis = 3)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","\n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv1,up9], axis = 3)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    # la couche suivante n'est pas pertinente pour du multiclasse\n","    # conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    # ici je doix changer conv10 pour avoir huit catégories, et donc utiliser softmax en activation\n","    conv10 = Conv2D(8, 1, activation='softmax')(conv9)\n","\n","    model = Model(inputs = inputs, outputs = conv10)\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=1e-4),\n","        loss='categorical_crossentropy',\n","        metrics=[sm.metrics.IOUScore()]\n","    )\n","    # model.summary()\n","\n","    if(pretrained_weights):\n","    \tmodel.load_weights(pretrained_weights)\n","\n","    return model"],"metadata":{"id":"qVaLG2NJ_kEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = Dataloader(\n","    data_folder=X_train_dir,\n","    batch_size=4,\n","    transform=val_transform,\n","    shuffle=True\n",")\n","\n","val_loader = Dataloader(\n","    data_folder=X_val_dir,\n","    batch_size=4,\n","    transform=val_transform,\n","    shuffle=False\n",")\n","\n","test_loader = Dataloader(\n","    data_folder=X_test_dir,\n","    batch_size=4,\n","    transform=val_transform,\n","    shuffle=False\n",")"],"metadata":{"id":"GdAYiCOfJtr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = unet()"],"metadata":{"id":"GKj7DzAOJ99r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imgs, masks = next(iter(train_loader))\n","print(\"images shape\", imgs.shape)\n","print(\"masks shape\", masks.shape)\n","print(\"mask unique\", np.unique(masks), masks.dtype)"],"metadata":{"id":"Y1LqAIFweC5A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Callbacks"],"metadata":{"id":"dhBAfVZgQ_pm"}},{"cell_type":"code","source":["earlystop_cb = EarlyStopping(\n","    monitor='val_loss',\n","    patience=8,\n","    restore_best_weights=True\n",")"],"metadata":{"id":"QLmWjSHjRCFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PerClassMetricsCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, val_loader, num_classes=8):\n","        # on initie d'abord le parent pour éviter les mauvaises surprises\n","        super().__init__()\n","        self.val_loader = val_loader\n","        self.num_classes = num_classes\n","        # on charge le dico des classes\n","        self.dicoclasses = dicoclasses or {i: f\"class_{i}\" for i in range(num_classes)}\n","        # on stocke les scores de chaque epoch dans un df\n","        self.df_scores = pd.DataFrame()\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # à chaque fin d'époque on calcule les métriques customs\n","        # d'abord on recueille les prédictions et la vérité\n","        all_preds = []\n","        all_trues = []\n","        for i in range(len(self.val_loader)):\n","            imgs, masks = self.val_loader[i]\n","            preds = self.model.predict(imgs, verbose=0)\n","            all_preds.append(preds)\n","            all_trues.append(masks)\n","        all_preds = np.concatenate(all_preds, axis=0)\n","        all_trues = np.concatenate(all_trues, axis=0)\n","        # on extrait la plus haute probabilité\n","        y_pred = np.argmax(all_preds, axis=-1)           # (n_samples, H, W)\n","        # on convertit aussi les ground truths en indices\n","        if all_trues.ndim == 4 and all_trues.shape[-1] > 1:\n","            y_true = np.argmax(all_trues, axis=-1)       # (n_samples, H, W)\n","        else:\n","            y_true = all_trues\n","\n","        dice_scores = []\n","        iou_scores = []\n","        for c in range(self.num_classes):\n","            y_true_c = (y_true == c).astype(np.int32)\n","            y_pred_c = (y_pred == c).astype(np.int32)\n","            intersection = (y_true_c * y_pred_c).sum()\n","            union = y_true_c.sum() + y_pred_c.sum()\n","            dice = (2. * intersection) / (union + 1e-6)\n","            dice_scores.append(dice)\n","\n","            union_iou = y_true_c.sum() + y_pred_c.sum() - intersection\n","            iou = (intersection) / (union_iou + 1e-6)\n","            iou_scores.append(iou)\n","\n","        # ajouts au dataframe de résultats\n","        row = {'epoch': epoch+1}\n","        for c, (d, iou) in enumerate(zip(dice_scores, iou_scores)):\n","            label = self.dicoclasses.get(c, f\"class_{c}\")\n","            row[f\"dice_{label}\"] = d\n","            row[f\"iou_{label}\"] = iou\n","        row['dice_mean'] = np.mean(dice_scores)\n","        row['iou_mean'] = np.mean(iou_scores)\n","\n","        self.df_scores = pd.concat([self.df_scores, pd.DataFrame([row])], ignore_index=True)\n","\n","\n","        print(f\"\\n=== Époque {epoch + 1} ===\")\n","        for c, (d, iou) in enumerate(zip(dice_scores, iou_scores)):\n","            className = dicoclasses.get(c, f\"class_{c}\")\n","            print(f\"{className} (classe {c}): Dice={d:.4f} | IoU={iou:.4f}\")\n","        print(f\"--- Dice moyen: {np.mean(dice_scores):.4f} | IoU moyen: {np.mean(iou_scores):.4f} ---\\n\")"],"metadata":{"id":"JLB24g8sAL9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["per_class_metrics_cb = PerClassMetricsCallback(val_loader=val_loader, num_classes=8)"],"metadata":{"id":"fVCOymLBAZKj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Je garde ce calcul \"artisanal\" que j'avais fait au départ sur une version différente, sans one-hot encoding. Cela me permettra de vérifier les déséquilibres de classe"],"metadata":{"id":"qSbOEQFTAexa"}},{"cell_type":"markdown","source":["## Fit et graphiques"],"metadata":{"id":"CjgmuRxdRXeI"}},{"cell_type":"code","source":["mlflow.set_experiment('BASELINE UNET SANS AUG')\n","with mlflow.start_run():\n","  history = model.fit(\n","  train_loader,\n","  validation_data=val_loader,\n","  epochs=50,\n","  callbacks=[earlystop_cb, per_class_metrics_cb],\n","  verbose=1\n","  )\n","  # Log des métriques epoch par epoch :\n","  for epoch, (loss, val_loss, iou, val_iou) in enumerate(zip(\n","      history.history['loss'],\n","      history.history['val_loss'],\n","      history.history['iou_score'],\n","      history.history['val_iou_score']\n","  )):\n","      mlflow.log_metric(\"loss\", loss, step=epoch)\n","      mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n","      mlflow.log_metric(\"iou_score\", iou, step=epoch)\n","      mlflow.log_metric(\"val_iou_score\", val_iou, step=epoch)\n","  # Sauvegarde de l'objet history complet :\n","  with open(\"history.pkl\", \"wb\") as f:\n","      pickle.dump(history.history, f)\n","  mlflow.log_artifact(\"history.pkl\")\n","\n","  # Instancie la métrique IoU fournie par segmentation_models\n","  iou_metric = sm.metrics.IOUScore()\n","\n","  start_time = time.time()\n","  n_samples = 0\n","\n","  all_preds, all_trues = [], []\n","\n","  for imgs, masks in test_loader:\n","      preds = model.predict(imgs)\n","      all_preds.append(preds)\n","      all_trues.append(masks)\n","      n_samples += imgs.shape[0]\n","\n","  y_pred = tf.concat(all_preds, axis=0)\n","  y_true = tf.concat(all_trues, axis=0)\n","\n","  elapsed_time = time.time() - start_time\n","  avg_inference_time = elapsed_time / n_samples\n","\n","  mean_iou_test = sm.metrics.iou_score(y_true, y_pred).numpy()\n","\n","  mlflow.log_metric(\"test_avg_inference_time\", avg_inference_time)\n","  mlflow.log_metric(\"test_mean_iou\", mean_iou_test)"],"metadata":{"id":"va3nfmp8J6Bz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","\n","plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n","plt.plot(history.history['loss'], label='train loss')\n","plt.plot(history.history['val_loss'], label='val loss')\n","plt.plot(history.history['iou_score'], label='train IoU_score')\n","plt.plot(history.history['val_iou_score'], label='val IoU_score')\n","plt.axhline(mean_iou_test, color='purple', linestyle='--', label=f'test IoU={mean_iou_test:.3f}')\n","\n","plt.legend()\n","plt.title('Baseline sans augmentation')\n","plt.xlabel('Époques')\n","plt.ylabel('Valeurs')\n","plt.tight_layout()\n","plt.savefig(os.path.join(imgPrezPath, \"base_wo_aug.svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)\n"],"metadata":{"id":"r2_UjgzIQprl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"OPgHJU8Wec3M"}},{"cell_type":"code","source":["model.save('/content/drive/MyDrive/Colab Notebooks/Projet 8/first_test.keras')"],"metadata":{"id":"J0LgH46SgrEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os._exit(0)"],"metadata":{"id":"OEzgB1aKu5EL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Premières prédictions"],"metadata":{"id":"mZs0nakO6jzI"}},{"cell_type":"code","source":["baseline_model = tf.keras.models.load_model(\n","    '/content/drive/MyDrive/Colab Notebooks/Projet 8/first_test.keras',\n","    compile=False    # On ne compile pas : on veut juste le modèle pour prédire\n",")"],"metadata":{"id":"5_at0jCQ4F4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["baseline_model = model"],"metadata":{"id":"Z9k09xCK4SG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = Dataloader(\n","    data_folder=X_test_dir,\n","    batch_size=1,           # Pour la démo, plus lisible\n","    transform=val_transform,\n","    shuffle=False\n",")"],"metadata":{"id":"qPwQpEAXYGV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, masks = test_loader[1]\n","image = images[0]\n","true_mask = masks[0]"],"metadata":{"id":"_jVPDJGTYKEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# on doit rajouter une dimension à l'image au début pour qu'elle ait la bonne forme pour entrer dans le modèle (batch)\n","pred = baseline_model.predict(np.expand_dims(image, 0))\n","# on réduit le mask de one hot à une image\n","pred_mask = np.argmax(pred[0], axis=-1)"],"metadata":{"id":"ACQ1P720Yl5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","fig, axs = plt.subplots(1, 2, figsize=(8, 5))\n","axs[0].imshow(image)\n","axs[0].set_title(\"Image test\")\n","axs[0].axis(\"off\")\n","axs[1].imshow(pred_mask, cmap=\"tab20\", vmin=0, vmax=(7))\n","axs[1].set_title(\"Mask prédit\")\n","axs[1].axis(\"off\")\n","fig.suptitle('Baseline sans augmentation')\n","plt.tight_layout()\n","plt.savefig(os.path.join(imgPrezPath, \"res_base_wo_aug.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"],"metadata":{"id":"OUJWnJbRZiVl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline avec augmentation"],"metadata":{"id":"SWukIlDbhsYu"}},{"cell_type":"code","source":["train_loader = Dataloader(\n","    data_folder=X_train_dir,\n","    batch_size=4,\n","    transform=train_transform,\n","    shuffle=True\n",")\n","\n","val_loader = Dataloader(\n","    data_folder=X_val_dir,\n","    batch_size=4,\n","    transform=val_transform,\n","    shuffle=False\n",")\n","\n","test_loader = Dataloader(\n","    data_folder=X_test_dir,\n","    batch_size=4,\n","    transform=val_transform,\n","    shuffle=False\n",")"],"metadata":{"id":"p8Zq6M3sh0_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["per_class_metrics_cb = PerClassMetricsCallback(val_loader=val_loader, num_classes=8)"],"metadata":{"id":"aUVzNoidmcLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2 = unet()"],"metadata":{"id":"VfAbT-JCiLiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlflow.set_experiment('BASELINE UNET AVEC AUG')\n","with mlflow.start_run():\n","  history2 = model2.fit(\n","      train_loader,\n","      validation_data=val_loader,\n","      epochs=50,\n","      callbacks=[earlystop_cb, per_class_metrics_cb],\n","      verbose=1\n","  )\n","  # Log des métriques epoch par epoch :\n","  for epoch, (loss, val_loss, iou, val_iou) in enumerate(zip(\n","      history2.history['loss'],\n","      history2.history['val_loss'],\n","      history2.history['iou_score'],\n","      history2.history['val_iou_score']\n","  )):\n","      mlflow.log_metric(\"loss\", loss, step=epoch)\n","      mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n","      mlflow.log_metric(\"iou_score\", iou, step=epoch)\n","      mlflow.log_metric(\"val_iou_score\", val_iou, step=epoch)\n","  # Sauvegarde de l'objet history complet :\n","  with open(\"history.pkl\", \"wb\") as f:\n","      pickle.dump(history2.history, f)\n","  mlflow.log_artifact(\"history.pkl\")\n","\n","  # Instancie la métrique IoU fournie par segmentation_models\n","  iou_metric = sm.metrics.IOUScore()\n","\n","  start_time = time.time()\n","  n_samples = 0\n","\n","  all_preds, all_trues = [], []\n","\n","  for imgs, masks in test_loader:\n","      preds = model2.predict(imgs)\n","      all_preds.append(preds)\n","      all_trues.append(masks)\n","      n_samples += imgs.shape[0]\n","\n","  y_pred = tf.concat(all_preds, axis=0)\n","  y_true = tf.concat(all_trues, axis=0)\n","\n","  elapsed_time = time.time() - start_time\n","  avg_inference_time = elapsed_time / n_samples\n","\n","  mean_iou_test = sm.metrics.iou_score(y_true, y_pred).numpy()\n","\n","  mlflow.log_metric(\"test_avg_inference_time\", avg_inference_time)\n","  mlflow.log_metric(\"test_mean_iou\", mean_iou_test)"],"metadata":{"id":"ksIEouk_iQQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","\n","plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n","plt.plot(history2.history['loss'], label='train loss')\n","plt.plot(history2.history['val_loss'], label='val loss')\n","plt.plot(history2.history['iou_score'], label='train IoU_score')\n","plt.plot(history2.history['val_iou_score'], label='val IoU_score')\n","plt.axhline(mean_iou_test, color='purple', linestyle='--', label=f'test IoU={mean_iou_test:.3f}')\n","\n","plt.legend()\n","plt.title('Baseline avec augmentation')\n","plt.xlabel('Époques')\n","plt.ylabel('Valeurs')\n","plt.tight_layout()\n","plt.savefig(os.path.join(imgPrezPath, \"base_w_aug2.svg\"), format=\"svg\", bbox_inches=\"tight\", pad_inches=0.1)"],"metadata":{"id":"fV4HIDDDiVih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2.save('/content/drive/MyDrive/Colab Notebooks/Projet 8/second_test.keras')"],"metadata":{"id":"48Mu4fQ-U9Xe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["baseline_model2 = tf.keras.models.load_model(\n","    '/content/drive/MyDrive/Colab Notebooks/Projet 8/second_test.keras',\n","    compile=False    # On ne compile pas : on veut juste le modèle pour prédire\n",")"],"metadata":{"id":"8oyEVLx29voR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, masks = test_loader[1]\n","image = images[0]\n","true_mask = masks[0]"],"metadata":{"id":"8NuVs4Ys930y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# on doit rajouter une dimension à l'image au début pour qu'elle ait la bonne forme pour entrer dans le modèle (batch)\n","pred = baseline_model2.predict(np.expand_dims(image, 0))\n","# on réduit le mask de one hot à une image\n","pred_mask = np.argmax(pred[0], axis=-1)"],"metadata":{"id":"fjWb6QLF-I2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Affichage\n","fig, axs = plt.subplots(1, 2, figsize=(8, 5))\n","axs[0].imshow(image)\n","axs[0].set_title(\"Image test\")\n","axs[0].axis(\"off\")\n","axs[1].imshow(pred_mask, cmap=\"tab20\", vmin=0, vmax=(7))\n","axs[1].set_title(\"Mask prédit\")\n","axs[1].axis(\"off\")\n","plt.tight_layout()\n","plt.tight_layout()\n","plt.savefig(os.path.join(imgPrezPath, \"res_base_w_aug.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"],"metadata":{"id":"v1q0EgX2-N-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_file = test_loader.indexes[1] + \"_leftImg8bit.png\""],"metadata":{"id":"lQGOLp1CDB1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_image = np.array(Image.open(image_file))"],"metadata":{"id":"5n3qdSKKDEr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_shape = original_image.shape[:2][::-1] # (width, height)"],"metadata":{"id":"UGUkMBS7HyXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_mask_resized = np.array(\n","    Image.fromarray(pred_mask.astype(np.uint8)).resize(\n","        original_shape, resample=Image.NEAREST\n","    )\n",")\n","\n","# Affichage\n","fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n","axs[0].imshow(original_image)\n","axs[0].set_title(\"Image d'origine\")\n","axs[0].axis(\"off\")\n","axs[1].imshow(pred_mask_resized, cmap=\"tab20\", vmin=0, vmax=7)\n","axs[1].set_title(\"Mask prédit (taille d'origine)\")\n","axs[1].axis(\"off\")\n","axs[2].imshow(image)\n","axs[2].set_title(\"Image prétraitée (entrée modèle)\")\n","axs[2].axis(\"off\")\n","plt.tight_layout()\n","plt.tight_layout()\n","plt.savefig(os.path.join(imgPrezPath, \"trio_base_pred.svg\"),format=\"svg\",bbox_inches=\"tight\",pad_inches=0.1,)"],"metadata":{"id":"5566VEEGDKQg"},"execution_count":null,"outputs":[]}]}